{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rUbEmuvZJxlI"
   },
   "source": [
    "# PyTorch - homework 2: neural networks\n",
    "\n",
    "-- Prof. Dorien Herremans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "efS07mO7J6AR"
   },
   "source": [
    "Please run the whole notebook with your code and submit the `.ipynb` file on eDimension that includes your answers [so after you run it]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mJpzFaX0J6Zz",
    "outputId": "33e49695-d65f-4948-f0ea-057c6f3ad787"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mHomework by Ivan Christian, number: 1003056\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "student_number=\"1003056\"\n",
    "student_name=\"Ivan Christian\"\n",
    "\n",
    "print(colored(\"Homework by \"  + student_name + ', number: ' + student_number,'red'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xDkwBg8LKQ_"
   },
   "source": [
    " ## Question 1 -- XOR neural network [3pts]\n",
    "\n",
    "a) Train an (at least) 2-layer neural network that can solve the XOR problem. \n",
    "\n",
    "b) Check the predictions resulting from your model in the second code box below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BINvhm-PLKak"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.9997462034225464, \n",
      "Epoch: 500, Loss: 0.0002214344567619264, \n",
      "Epoch: 1000, Loss: 0.00012036479165544733, \n",
      "Epoch: 1500, Loss: 8.845984120853245e-05, \n",
      "Epoch: 2000, Loss: 6.330659380182624e-05, \n",
      "Epoch: 2500, Loss: 8.215211710194126e-05, \n",
      "Epoch: 3000, Loss: 0.9999990463256836, \n",
      "Epoch: 3500, Loss: 3.3376607461832464e-05, \n",
      "Epoch: 4000, Loss: 2.8901149562443607e-05, \n",
      "Epoch: 4500, Loss: 0.9999994039535522, \n",
      "Epoch: 5000, Loss: 0.9999995231628418, \n",
      "Epoch: 5500, Loss: 2.112330730597023e-05, \n",
      "Epoch: 6000, Loss: 1.9251523553975858e-05, \n",
      "Epoch: 6500, Loss: 1.9294542653369717e-05, \n",
      "Epoch: 7000, Loss: 2.9500253731384873e-05, \n",
      "Epoch: 7500, Loss: 1.5546536815236323e-05, \n",
      "Epoch: 8000, Loss: 1.4926215953892097e-05, \n"
     ]
    }
   ],
   "source": [
    "# load your data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = torch.Tensor([[0,0],[0,1], [1,0], [1,1]])\n",
    "Y = torch.Tensor([[0,1,1,0]]).view(-1,1)\n",
    "\n",
    "# name your model xor\n",
    "\n",
    "\n",
    "class xor(nn.Module):\n",
    "    def __init__(self, input_dim = 2, output_dim=1):\n",
    "        super(xor, self).__init__()\n",
    "        self.lin1 = nn.Linear(input_dim, 512)\n",
    "        self.lin2 = nn.Linear(512, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = xor().cuda()\n",
    "\n",
    "# define your model loss function, optimizer, etc. \n",
    "device = 'cuda'\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "\n",
    "\n",
    "def weights_init(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # initialize the weight tensor, here we use a normal distribution\n",
    "            m.weight.data.normal_(0, 1)\n",
    "weights_init(model)\n",
    "\n",
    "# train the model\n",
    "def train(x, y, model, loss_func, optimizer, device, epochs=8001):\n",
    "    steps = x.size(0)\n",
    "    for epoch in range(epochs):\n",
    "        for j in range(steps):\n",
    "            data_point = np.random.randint(x.size(0))\n",
    "            x_var = Variable(x[data_point], requires_grad=False).to(device)\n",
    "            y_var = Variable(y[data_point], requires_grad=False).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x_var)\n",
    "            loss = loss_func(y_hat, y_var)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"Epoch: {epoch}, Loss: {loss.cpu().data.numpy()}, \")\n",
    "train(X, Y, model, loss_func, optimizer, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "51Ra1T6n2r_R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 xor 0 = 1\n",
      "0 xor 1 = 1\n",
      "1 xor 0 = 0\n",
      "1 xor 1 = 0\n"
     ]
    }
   ],
   "source": [
    "# test your model using the following functions (make sure the output is printed and saved when you submit this notebook):\n",
    "# depending on how you defined your network you may need to slightly tweek the below prediction function\n",
    "\n",
    "test = [[0,0],[0,1], [1,0], [1,1]]\n",
    "\n",
    "\n",
    "for trial in test: \n",
    "    Xtest = torch.Tensor(trial).to(device)\n",
    "    model = xor().to(device)\n",
    "    y_hat = model(Xtest).cuda()\n",
    "    prediction = 1 if y_hat[0] > 0.5 else 0\n",
    "    print(\"{0} xor {1} = {2}\".format(int(Xtest[0]), int(Xtest[1]), prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pqIqD5ZzyUOW"
   },
   "source": [
    "## Question 2  [2pts]\n",
    "\n",
    "Imagine a neural network model for a multilabel classification task. \n",
    "\n",
    "a) Which loss function should you use?\n",
    "\n",
    "b) The resulting trained modal has a high variance error. Give 4 possible solutions to improve the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hzye9G18PQ0c"
   },
   "source": [
    "```\n",
    "* answer A\n",
    "a) Binary cross entropy loss\n",
    "* answer B\n",
    "  - 1\n",
    "  - 2\n",
    "  - ...\n",
    "b) High Variance Error Possible (Overfitting) solutions:\n",
    "- 1 : Dropout \n",
    "- 2 : Data augmentation during training time\n",
    "- 3 : Early Stopping\n",
    "- 4 : Get more training data\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FcceOSnjjSHf"
   },
   "source": [
    "## Question 3 - Improve hit classification [5pts]\n",
    "\n",
    "Remember the hit predicton dataset from last week? \n",
    "\n",
    "a) Improve the model using a multiplayer perceptron. \n",
    "\n",
    "b) Make sure to run your models on the GPU. \n",
    "\n",
    "c) Tweek the hyperparameters such as number of nodes or layers, or other. Show two possible configurations and explain which works better and very briefly explain why this may be the case. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t-jkJDTdjSRX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\chris\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (3.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([321])) that is different to the input size (torch.Size([321, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.6820421814918518, \n",
      "Epoch: 500, Loss: 0.6343249082565308, \n",
      "Epoch: 1000, Loss: 0.6017062664031982, \n",
      "Epoch: 1500, Loss: 0.5698546171188354, \n",
      "Epoch: 2000, Loss: 0.5402866005897522, \n",
      "Epoch: 2500, Loss: 0.5157894492149353, \n",
      "Epoch: 3000, Loss: 0.4952634871006012, \n",
      "Epoch: 3500, Loss: 0.4757671058177948, \n",
      "Epoch: 4000, Loss: 0.45520785450935364, \n",
      "Epoch: 4500, Loss: 0.43378180265426636, \n",
      "Epoch: 5000, Loss: 0.4119996130466461, \n",
      "Epoch: 5500, Loss: 0.39112672209739685, \n",
      "Epoch: 6000, Loss: 0.3720206320285797, \n",
      "Epoch: 6500, Loss: 0.3553045094013214, \n",
      "Epoch: 7000, Loss: 0.34097975492477417, \n",
      "Epoch: 7500, Loss: 0.32933303713798523, \n",
      "Epoch: 8000, Loss: 0.3203296959400177, \n",
      "Epoch: 8500, Loss: 0.31262582540512085, \n",
      "Epoch: 9000, Loss: 0.30709490180015564, \n"
     ]
    }
   ],
   "source": [
    "# code your model 1\n",
    "# Perceptron structure --> fc -> relu\n",
    "\n",
    "!pip install wget\n",
    "import wget\n",
    "import os\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "train_url = 'https://dorax.s3.ap-south-1.amazonaws.com/herremans_hit_1030training.csv'\n",
    "test_url = 'https://dorax.s3.ap-south-1.amazonaws.com/herremans_hit_1030test.csv'\n",
    "\n",
    "\n",
    "if not os.path.exists('herremans_hit_1030training.csv') and not os.path.exists('herremans_hit_1030test.csv'):\n",
    "    train_data = wget.download(train_url) \n",
    "    test_data = wget.download(test_url)\n",
    "else:\n",
    "    train_data = 'herremans_hit_1030training.csv'\n",
    "    test_data = 'herremans_hit_1030test.csv'\n",
    "\n",
    "# load data\n",
    "\n",
    "train_data = pd.read_csv(train_data)\n",
    "test_data = pd.read_csv(test_data)\n",
    "\n",
    "# define logistic regression model\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "  # input_size: Dimensionality of input feature vector.\n",
    "  # num_classes: The number of classes in the classification problem.\n",
    "    def __init__(self, input_size, num_classes):\n",
    "    # Always call the superclass (nn.Module) constructor first!\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, 128)\n",
    "        self.l2 = nn.Linear(128, 64)\n",
    "        self.l3 = nn.Linear(64, num_classes)\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = torch.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.l3(out)\n",
    "        out = torch.relu(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# train model\n",
    "\n",
    "device = 'cuda'\n",
    "epochs = 9001\n",
    "\n",
    "num_outputs = 1\n",
    "num_input_features = 49\n",
    "\n",
    "lr_rate = 0.005\n",
    "loss_function = nn.BCELoss()\n",
    "model1 = LogisticRegression(num_input_features, num_outputs).to(device)\n",
    "optimizer = torch.optim.SGD(model1.parameters(), lr=lr_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    x_data = torch.FloatTensor(train_data.loc[:, train_data.columns != 'Topclass1030'].values).to(device)\n",
    "    y_true = torch.FloatTensor(train_data['Topclass1030']).to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    y_pred = model1(x_data)\n",
    "    \n",
    "    loss = loss_function(y_pred, y_true).to(device) #calculate the loss\n",
    "    loss.backward() #backprop\n",
    "    optimizer.step()\n",
    "    if epoch % 500 == 0:\n",
    "        print (\"Epoch: {0}, Loss: {1}, \".format(epoch,loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UIDPTKcFkETc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 45, True Negatives: 14\n",
      "False Positives: 15, False Negatives: 5\n",
      "Class specific accuracy of correctly predicting a hit song is 0.9\n"
     ]
    }
   ],
   "source": [
    "# evaluate model 1 (called model1 here)\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "def run_evaluation(my_model):\n",
    "    '''\n",
    "    Function to evaluate the performance of the custom model\n",
    "    \n",
    "    Args:\n",
    "    - my_model : model (custom model to be tested)\n",
    "    returns:\n",
    "    - None\n",
    "    '''\n",
    "\n",
    "    device = 'cuda'\n",
    "    my_model.eval()\n",
    "\n",
    "    test = pd.read_csv('herremans_hit_1030test.csv')\n",
    "    labels = test.iloc[:,-1]\n",
    "    test = test.drop('Topclass1030', axis=1)\n",
    "    testdata = torch.Tensor(test.values).to(device)\n",
    "    testlabels = torch.Tensor(labels.values).view(-1,1).to(device)\n",
    "\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    FP = 0\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(0, testdata.size()[0]): \n",
    "        Xtest = testdata[i]\n",
    "        y_hat = my_model(Xtest)\n",
    "        if y_hat > 0.5:\n",
    "            prediction = 1\n",
    "        else: \n",
    "            prediction = 0\n",
    "\n",
    "        if (prediction == testlabels[i]):\n",
    "            if (prediction == 1):\n",
    "                TP += 1\n",
    "            else: \n",
    "                TN += 1\n",
    "\n",
    "        else:\n",
    "            if (prediction == 1):\n",
    "                FP += 1\n",
    "            else: \n",
    "                FN += 1\n",
    "\n",
    "    print(\"True Positives: {0}, True Negatives: {1}\".format(TP, TN))\n",
    "    print(\"False Positives: {0}, False Negatives: {1}\".format(FP, FN))\n",
    "    rate = TP/(FN+TP)\n",
    "    print(\"Class specific accuracy of correctly predicting a hit song is {0}\".format(rate))\n",
    "\n",
    "run_evaluation(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xghPDDNmkHn2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\chris\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (3.2)\n",
      "Epoch: 0, Loss: 0.6893376111984253, \n",
      "Epoch: 500, Loss: 0.6533893346786499, \n",
      "Epoch: 1000, Loss: 0.6706206202507019, \n",
      "Epoch: 1500, Loss: 0.6420188546180725, \n",
      "Epoch: 2000, Loss: 0.6372660398483276, \n",
      "Epoch: 2500, Loss: 0.6195134520530701, \n",
      "Epoch: 3000, Loss: 0.6013129949569702, \n",
      "Epoch: 3500, Loss: 0.6084885597229004, \n",
      "Epoch: 4000, Loss: 0.5967109203338623, \n",
      "Epoch: 4500, Loss: 0.5848504900932312, \n",
      "Epoch: 5000, Loss: 0.5726922154426575, \n",
      "Epoch: 5500, Loss: 0.5605628490447998, \n",
      "Epoch: 6000, Loss: 0.5519987940788269, \n",
      "Epoch: 6500, Loss: 0.5650646090507507, \n",
      "Epoch: 7000, Loss: 0.548926830291748, \n",
      "Epoch: 7500, Loss: 0.5453125238418579, \n",
      "Epoch: 8000, Loss: 0.5360672473907471, \n",
      "Epoch: 8500, Loss: 0.5202764868736267, \n",
      "Epoch: 9000, Loss: 0.5404687523841858, \n"
     ]
    }
   ],
   "source": [
    "# code your model 2\n",
    "!pip install wget\n",
    "import wget\n",
    "import os\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "train_url = 'https://dorax.s3.ap-south-1.amazonaws.com/herremans_hit_1030training.csv'\n",
    "test_url = 'https://dorax.s3.ap-south-1.amazonaws.com/herremans_hit_1030test.csv'\n",
    "\n",
    "\n",
    "if not os.path.exists('herremans_hit_1030training.csv') and not os.path.exists('herremans_hit_1030test.csv'):\n",
    "    train_data = wget.download(train_url) \n",
    "    test_data = wget.download(test_url)\n",
    "else:\n",
    "    train_data = 'herremans_hit_1030training.csv'\n",
    "    test_data = 'herremans_hit_1030test.csv'\n",
    "\n",
    "# load data\n",
    "\n",
    "train_data = pd.read_csv(train_data)\n",
    "test_data = pd.read_csv(test_data)\n",
    "\n",
    "# define logistic regression model\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "  # input_size: Dimensionality of input feature vector.\n",
    "  # num_classes: The number of classes in the classification problem.\n",
    "    def __init__(self, input_size, num_classes, dropout=0.6):\n",
    "    # Always call the superclass (nn.Module) constructor first!\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, 512)\n",
    "        self.l2 = nn.Linear(512, 256)\n",
    "        self.l3 = nn.Linear(256, 64)\n",
    "        self.l4 = nn.Linear(64, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = torch.relu(out)\n",
    "\n",
    "        out = self.l2(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.l3(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.l4(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# train model\n",
    "\n",
    "device = 'cuda'\n",
    "epochs = 9001\n",
    "\n",
    "num_outputs = 1\n",
    "num_input_features = 49\n",
    "\n",
    "lr_rate = 0.005\n",
    "loss_function = nn.BCELoss()\n",
    "model2 = LogisticRegression(num_input_features, num_outputs).to(device)\n",
    "optimizer = torch.optim.SGD(model2.parameters(), lr=lr_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    x_data = torch.FloatTensor(train_data.loc[:, train_data.columns != 'Topclass1030'].values).to(device)\n",
    "    y_true = torch.FloatTensor(train_data['Topclass1030']).to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    y_pred = model2(x_data)\n",
    "    \n",
    "    loss = loss_function(y_pred, y_true).to(device) #calculate the loss\n",
    "    loss.backward() #backprop\n",
    "    optimizer.step()\n",
    "    if epoch % 500 == 0:\n",
    "        print (\"Epoch: {0}, Loss: {1}, \".format(epoch,loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wAIifiHJkHyW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 47, True Negatives: 13\n",
      "False Positives: 16, False Negatives: 3\n",
      "Class specific accuracy of correctly predicting a hit song is 0.94\n"
     ]
    }
   ],
   "source": [
    "# evaluate model 2 (called model2 here)\n",
    "\n",
    "run_evaluation(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QPsxbT0KkGs1"
   },
   "source": [
    "Which works better and why do you think this may be (very briefly)? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6GzjI77HkSwH"
   },
   "source": [
    "**[your answer here, also please summarise the differences between your two models]**\n",
    "\n",
    "Model 1 is a 3-layer multilayer perceptron model\n",
    "\n",
    "Model 2 is a 4-layer multilayer perceptron model with dropouts after layer 3 and 4. Layers 2 (512 nodes), Layers 3 (256 nodes) and Layers 4 (64 nodes) are upgraded versions of model which only has Layer 2 (128 nodes) and Layer 3 (64 nodes). This makes model 2 a more complicated model hence making it better than the original model.\n",
    "\n",
    "Model 2 is better because of the following reasons:\n",
    "\n",
    "- Dropout makes the model more generalisable and it results \n",
    "- more layers mean that the model will be able to learn more complex features. As such, there is an increase in the test accuracy when using the model 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hh5O8qS_khug"
   },
   "source": [
    "Additionally, submit your results [here](https://forms.gle/NtJJEE7Wm5ZRM3Je7) for 'Class specific accuracy of correctly predicting a hit song' and see if you got the best performance of the class! Good luck!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AI homework 2 - neural networks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
